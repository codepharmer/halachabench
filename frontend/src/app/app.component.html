<div class="page" *ngIf="vm$ | async as vm; else loading">
  <section class="hero">
    <div class="container">
      <div class="hero-content">
        <h1 class="title">HalachaBench</h1>
        <h2 class="subtitle">
          A LiveBench-inspired, contamination-aware benchmark for halacha QA with
          deterministic scoring.
        </h2>
        <p class="hero-note">
          Objective, closed-format questions with explicit source attribution and
          reproducible evaluation.
        </p>
        <div class="hero-links">
          <a class="button primary" href="#leaderboard">Leaderboard</a>
          <a class="button" href="#details">Details</a>
          <a
            class="button"
            href="assets/releases/2026-02-05/questions.jsonl"
            download
          >
            Data
          </a>
          <a class="button" href="assets/results.sample.json" download>Report</a>
        </div>
        <div class="hero-meta">
          <div class="meta-item">
            <span>Release</span>
            <strong>{{ vm.releaseId }}</strong>
          </div>
          <div class="meta-item">
            <span>Questions</span>
            <strong>{{ vm.nQuestions }}</strong>
          </div>
          <div class="meta-item">
            <span>Categories</span>
            <strong>{{ vm.categoryCount }}</strong>
          </div>
          <div class="meta-item">
            <span>Missing</span>
            <strong>{{ vm.nMissing }}</strong>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="details">
    <div class="container narrow">
      <h3>Introduction</h3>
      <p>
        HalachaBench focuses on objectively scorable halacha questions. It is
        designed to keep evaluation contamination in check by shipping versioned
        releases, while remaining fully reproducible.
      </p>
      <ul class="bullet-list">
        <li>Deterministic scoring rules with no judge model.</li>
        <li>Closed-format answers with explicit source references.</li>
        <li>Versioned releases to control contamination.</li>
        <li>Public artifacts and reproducible evaluation output.</li>
      </ul>
      <p class="callout">
        Want to evaluate your model on HalachaBench? Open an issue and include
        predictions in the JSONL format.
      </p>
    </div>
  </section>

  <section class="section" id="leaderboard">
    <div class="container">
      <div class="section-header">
        <div>
          <h3>Leaderboard</h3>
          <p>
            We refresh the benchmark on a release cadence to reduce
            contamination. The seed release is
            <strong>{{ vm.releaseId }}</strong>.
          </p>
        </div>
        <div class="overview-card">
          <div class="overview-label">Overall score</div>
          <div class="overview-value">{{ vm.overall | number: '1.3-3' }}</div>
        </div>
      </div>

      <div class="slider-wrap">
        <input
          class="slider"
          type="range"
          [min]="0"
          [max]="releaseDates.length - 1"
          [value]="selectedReleaseIndex"
          (input)="onReleaseChange($event)"
        />
        <div class="slider-bubble" [style.left.%]="sliderPercent">
          {{ releaseDates[selectedReleaseIndex] }}
        </div>
      </div>

      <div class="table-container">
        <table class="leaderboard-table">
          <thead>
            <tr>
              <th>Model</th>
              <th>Overall</th>
              <th *ngFor="let header of vm.categoryHeaders">{{ header }}</th>
            </tr>
          </thead>
          <tbody>
            <tr *ngFor="let row of vm.leaderboardRows">
              <td class="model-cell">
                <div class="model-name">{{ row.name }}</div>
                <div class="model-sub">{{ row.subtitle }}</div>
              </td>
              <td class="score">{{ row.overall | number: '1.3-3' }}</td>
              <td *ngFor="let header of vm.categoryHeaders">
                {{ row.categoryScores[header] | number: '1.3-3' }}
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h3>Category breakdown</h3>
      <div class="category-grid">
        <div class="category-card" *ngFor="let category of vm.categories">
          <div class="category-header">
            <div>
              <h4>{{ category.name }}</h4>
              <span>{{ category.nTasks }} tasks</span>
            </div>
            <div class="category-score">
              {{ category.score | number: '1.3-3' }}
            </div>
          </div>
          <div class="task-row" *ngFor="let task of category.tasks">
            <span>{{ task.name }}</span>
            <span class="task-score">{{ task.score | number: '1.3-3' }}</span>
            <span class="task-count">n={{ task.nQuestions }}</span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h3>Sample questions</h3>
      <div class="question-grid">
        <div class="question-card" *ngFor="let question of vm.sampleQuestions">
          <div class="question-meta">
            {{ question.category }} / {{ question.task }}
          </div>
          <p>{{ question.prompt }}</p>
          <div class="question-footer">
            <span class="chip">{{ question.answerType }}</span>
            <span class="hash">{{ question.id | slice: 0:10 }}...</span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="bibtex">
    <div class="container narrow">
      <h3>BibTeX</h3>
      <pre class="bibtex"><code>@misc{halachabench2026,
  title = {HalachaBench: Objective Halacha QA Benchmark},
  year = {2026},
  howpublished = {Seed release {{ vm.releaseId }}},
  note = {Deterministic scoring, closed-format tasks}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container narrow">
      <p>
        HalachaBench is a benchmark. It does not provide halachic guidance.
        Always consult qualified authorities and primary sources.
      </p>
      <p class="footer-note">
        Forked from LiveBench (livebench.ai). Repository:
        <a
          href="https://github.com/LiveBench/LiveBench"
          target="_blank"
          rel="noopener"
        >
          LiveBench/LiveBench
        </a>
      </p>
    </div>
  </footer>
</div>

<ng-template #loading>
  <div class="page">
    <div class="loading">Loading benchmark assets...</div>
  </div>
</ng-template>